{"cells":[{"cell_type":"markdown","id":"292d2d29","metadata":{"id":"292d2d29"},"source":["# Insight to Impact: Seeing Through Another Eyes\n","**Hands‑on A (20 minutes)**\n","\n","This mini‑lab lets you _feel_ visual limitations first (color‑vision deficiency / low‑vision) and then see how modern open‑vocabulary perception + Color Vision Deficiency(CVD)‑aware + optional text-to-speech(TTS) can restore information.\n","\n","1) **Experience** CVD simulation (what some users actually see).  \n","2) **Assist** with AI: open‑vocabulary detection → promptable segmentation → high‑contrast overlays.  \n","3) **Create** accessibility: make small edits (palette / TTS / blur) to help people see better.\n","\n","**Backends (switchable)**: **[OmDet‑Turbo](https://github.com/om-ai-lab/OmDet)** (default, fastest) · **[Florence‑2](https://huggingface.co/microsoft/Florence-2-large) (experimental)** · **[Grounding‑DINO](https://github.com/IDEA-Research/GroundingDINO) (baseline)**  \n","\n","**Segmentation**: **SAM v1**\n","\n","**UI**: Gradio (image upload/webcam), prompt text, CVD type, TTS, exports."]},{"cell_type":"markdown","id":"077aa479","metadata":{"id":"077aa479"},"source":["## 0. Runtime setup"]},{"cell_type":"code","execution_count":null,"id":"b559c017","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b559c017","executionInfo":{"status":"ok","timestamp":1761217195371,"user_tz":-120,"elapsed":393922,"user":{"displayName":"Yutong Zhou","userId":"15287789925998213235"}},"outputId":"74b0df67-cdcc-46f6-ffc0-bfffcbdc82a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'GroundingDINO' already exists and is not an empty directory.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","fatal: destination path 'sam2' already exists and is not an empty directory.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Tip: Use a GPU runtime (Runtime → Change runtime type → GPU).\n","# Keep installs minimal to stay within 15 minutes.\n","\n","# Core\n","%pip -q install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n","%pip -q install opencv-python pillow matplotlib numpy scipy gTTS gradio ultralytics supervision\n","\n","# Transformers for OmDet-Turbo / Florence-2\n","%pip -q install --upgrade transformers accelerate timm huggingface_hub\n","\n","# GroundingDINO\n","!git clone -q https://github.com/IDEA-Research/GroundingDINO.git\n","%pip -q install -e GroundingDINO\n","\n","# SAM 2 + fallback SAM v1\n","!git clone -q https://github.com/facebookresearch/sam2.git\n","%pip -q install -e sam2\n","%pip -q install git+https://github.com/facebookresearch/segment-anything.git"]},{"cell_type":"markdown","id":"4cb7fcab","metadata":{"id":"4cb7fcab"},"source":["## 1. Download weights (fast and small)"]},{"cell_type":"code","execution_count":null,"id":"eb5abf1a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eb5abf1a","executionInfo":{"status":"ok","timestamp":1761217195402,"user_tz":-120,"elapsed":8,"user":{"displayName":"Yutong Zhou","userId":"15287789925998213235"}},"outputId":"212b5ba0-8838-4fe6-dfdc-ab1b4c41a7ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Weights ready\n"]}],"source":["import os, urllib.request, pathlib\n","\n","WEIGHTS = pathlib.Path(\"weights\"); WEIGHTS.mkdir(exist_ok=True)\n","\n","# Grounding-DINO (Swin-T OGC)\n","GDINO_URL = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\"\n","GDINO_PATH = WEIGHTS / \"groundingdino_swint_ogc.pth\"\n","if not GDINO_PATH.exists():\n","    print(\"Downloading GroundingDINO weights…\")\n","    urllib.request.urlretrieve(GDINO_URL, GDINO_PATH)\n","\n","# SAM2 tiny\n","SAM2_URL = \"https://huggingface.co/facebook/sam2-hiera-tiny/resolve/main/sam2_hiera_tiny.pt\"\n","SAM2_PATH = WEIGHTS / \"sam2_hiera_tiny.pt\"\n","if not SAM2_PATH.exists():\n","    print(\"Downloading SAM2 (tiny) weights…\")\n","    urllib.request.urlretrieve(SAM2_URL, SAM2_PATH)\n","\n","# SAM v1 ViT-B\n","SAMV1_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n","SAMV1_PATH = WEIGHTS / \"sam_vit_b_01ec64.pth\"\n","if not SAMV1_PATH.exists():\n","    print(\"Downloading SAM v1 (ViT-B) weights…\")\n","    urllib.request.urlretrieve(SAMV1_URL, SAMV1_PATH)\n","\n","print(\"✓ Weights ready\")"]},{"cell_type":"markdown","id":"9c998795","metadata":{"id":"9c998795"},"source":["## 2. Feel it first: CVD simulation (approximate)"]},{"cell_type":"code","execution_count":null,"id":"5fac43dd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fac43dd","executionInfo":{"status":"ok","timestamp":1761217195429,"user_tz":-120,"elapsed":25,"user":{"displayName":"Yutong Zhou","userId":"15287789925998213235"}},"outputId":"33bb884d-7033-496a-dc32-81c57dfcb816"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded CVD simulation utilities. Use the Gradio UI to try it interactively.\n"]}],"source":["import numpy as np, cv2\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","def simulate_cvd(image_rgb, mode=\"deutan\"):\n","    import numpy as np\n","    img = (image_rgb.astype(np.float32) / 255.0)\n","    if mode==\"protan\":\n","        M = np.array([[0.567,0.433,0],[0.558,0.442,0],[0,0.242,0.758]])\n","    elif mode==\"deutan\":\n","        M = np.array([[0.625,0.375,0],[0.7,0.3,0],[0,0.3,0.7]])\n","    elif mode==\"tritan\":\n","        M = np.array([[0.95,0.05,0],[0,0.433,0.567],[0,0.475,0.525]])\n","    else:\n","        M = np.eye(3)\n","    return (np.clip(img @ M.T, 0, 1) * 255).astype(np.uint8)\n","\n","print(\"Loaded CVD simulation utilities. Use the Gradio UI to try it interactively.\")"]},{"cell_type":"markdown","id":"b4edb7ea","metadata":{"id":"b4edb7ea"},"source":["## 3. Models: Detector backend + SAM v1"]},{"cell_type":"code","execution_count":null,"id":"66ce4a29","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66ce4a29","executionInfo":{"status":"ok","timestamp":1761217214053,"user_tz":-120,"elapsed":18622,"user":{"displayName":"Yutong Zhou","userId":"15287789925998213235"}},"outputId":"162fa597-102b-4977-ab86-d8190e0debc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["final text_encoder_type: bert-base-uncased\n","Using SAM v1 on cpu.\n"]}],"source":["import sys, os\n","sys.path.append(os.path.abspath(\"GroundingDINO\"))\n","\n","import torch, torch.nn.functional as F, cv2, numpy as np\n","from PIL import Image\n","import groundingdino.datasets.transforms as T\n","from groundingdino.models import build_model\n","from groundingdino.util.slconfig import SLConfig\n","from groundingdino.util.utils import clean_state_dict\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# --- Grounding-DINO ---\n","def load_groundingdino(cfg_path, ckpt_path):\n","    args = SLConfig.fromfile(cfg_path)\n","    model = build_model(args)\n","    checkpoint = torch.load(str(ckpt_path), map_location=\"cpu\")\n","    model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n","    model.eval()\n","    return model.to(device)\n","\n","GDINO_CFG = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n","GDINO_PATH = \"weights/groundingdino_swint_ogc.pth\"\n","gdino = load_groundingdino(GDINO_CFG, GDINO_PATH)\n","\n","# ---- SAM v1 ----\n","import torch, os\n","from segment_anything import sam_model_registry, SamPredictor\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","SAMV1_PATH = \"weights/sam_vit_b_01ec64.pth\"\n","\n","sam = sam_model_registry[\"vit_b\"](checkpoint=SAMV1_PATH)\n","sam.to(device)\n","sam_predictor = SamPredictor(sam)\n","\n","use_sam2 = False\n","print(f\"Using SAM v1 on {device}.\")"]},{"cell_type":"markdown","id":"ad51ab8d","metadata":{"id":"ad51ab8d"},"source":["## 4. Detection (OmDet‑Turbo / Florence‑2 experimental / Grounding‑DINO)"]},{"cell_type":"code","execution_count":null,"id":"c11d5085","metadata":{"id":"c11d5085"},"outputs":[],"source":["import time, re\n","from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n","\n","# --- OmDet‑Turbo (default) ---\n","om_processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n","om_model = AutoModelForZeroShotObjectDetection.from_pretrained(\n","    \"omlab/omdet-turbo-swin-tiny-hf\",\n","    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n","    device_map=\"auto\"\n",").eval()\n","\n","def omdet_detect(image_rgb, prompt_text, box_threshold=0.25):\n","    queries = [q.strip() for q in prompt_text.split(\",\") if q.strip()]\n","    if len(queries) == 0:\n","        return np.zeros((0,4), dtype=int), np.zeros((0,), dtype=float), []\n","    inputs = om_processor(text=queries, images=image_rgb, return_tensors=\"pt\").to(om_model.device)\n","    with torch.inference_mode(), torch.autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n","        outputs = om_model(**inputs)\n","    target_sizes = torch.tensor([image_rgb.shape[:2]], device=om_model.device)\n","    results = om_processor.post_process_grounded_object_detection(outputs, threshold=box_threshold, target_sizes=target_sizes)[0]\n","    boxes  = results[\"boxes\"].detach().cpu().numpy().astype(int) if \"boxes\" in results else np.zeros((0,4), dtype=int)\n","    scores = results[\"scores\"].detach().cpu().numpy() if \"scores\" in results else np.zeros((0,), dtype=float)\n","    labels = [queries[i % len(queries)] for i in range(len(boxes))] if len(boxes)>0 else []\n","    return boxes, scores, labels\n","\n","# --- Grounding‑DINO ---\n","def gdino_detect(image_bgr, text_prompt, box_threshold=0.25, text_threshold=0.25):\n","    H, W = image_bgr.shape[:2]\n","    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n","    image_pil = Image.fromarray(image_rgb)\n","\n","    transform = T.Compose([\n","        T.RandomResize([800], max_size=1333),\n","        T.ToTensor(),\n","        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","    ])\n","    image, _ = transform(image_pil, None)\n","    with torch.no_grad():\n","        outputs = gdino(image[None].to(device), captions=[text_prompt])\n","    logits = outputs[\"pred_logits\"].cpu().sigmoid()[0]\n","    boxes = outputs[\"pred_boxes\"].cpu()[0]\n","    filt = logits.max(dim=1).values > box_threshold\n","    boxes = boxes[filt]; scores = logits[filt].max(dim=1).values\n","\n","    if boxes.numel()==0:\n","        return np.zeros((0,4), dtype=int), np.zeros((0,), dtype=float), []\n","\n","    cxcywh = boxes.numpy()\n","    xyxy = np.zeros_like(cxcywh)\n","    xyxy[:,0] = (cxcywh[:,0] - cxcywh[:,2]/2.0) * W\n","    xyxy[:,1] = (cxcywh[:,1] - cxcywh[:,3]/2.0) * H\n","    xyxy[:,2] = (cxcywh[:,0] + cxcywh[:,2]/2.0) * W\n","    xyxy[:,3] = (cxcywh[:,1] + cxcywh[:,3]/2.0) * H\n","    xyxy = np.clip(xyxy, 0, [W-1, H-1, W-1, H-1]).astype(np.int32)\n","\n","    labels = [s.strip() for s in text_prompt.split(\",\") if s.strip()] or [\"object\"]\n","    return xyxy, scores.numpy(), labels[:len(xyxy)]\n","\n","# --- Florence‑2 (experimental; safe no-op if unavailable) ---\n","try:\n","    from transformers import AutoProcessor as FProcessor, AutoModelForCausalLM as FModel\n","    fl_processor = FProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n","    fl_model = FModel.from_pretrained(\n","        \"microsoft/Florence-2-base\",\n","        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n","        device_map=\"auto\", trust_remote_code=True\n","    ).eval()\n","    FLORENCE_READY = True\n","except Exception as e:\n","    FLORENCE_READY = False\n","    fl_processor = None; fl_model = None\n","\n","def florence2_detect(image_rgb, prompt_text):\n","    if not FLORENCE_READY:\n","        return np.zeros((0,4), dtype=int), np.zeros((0,), dtype=float), []\n","    task_prefix = \"<OPEN_VOCAB_DET>\"\n","    inputs = fl_processor(text=task_prefix + prompt_text, images=image_rgb, return_tensors=\"pt\").to(fl_model.device)\n","    with torch.inference_mode():\n","        ids = fl_model.generate(**inputs, max_new_tokens=256)\n","    text = fl_processor.batch_decode(ids, skip_special_tokens=False)[0]\n","    boxes = []\n","    for m in re.finditer(r\"\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\", text):\n","        x1,y1,x2,y2 = map(int, m.groups())\n","        boxes.append([x1,y1,x2,y2])\n","    boxes = np.array(boxes, dtype=int) if boxes else np.zeros((0,4), dtype=int)\n","    scores = np.ones((len(boxes),), dtype=float) if len(boxes)>0 else np.zeros((0,), dtype=float)\n","    labels = [s.strip() for s in prompt_text.split(\",\") if s.strip()] or [\"object\"]\n","    return boxes, scores, labels[:len(boxes)]"]},{"cell_type":"markdown","id":"bfcedd83","metadata":{"id":"bfcedd83"},"source":["## 5. Segmentation + CVD‑aware overlays + TTS"]},{"cell_type":"code","execution_count":null,"id":"4a30d46c","metadata":{"id":"4a30d46c"},"outputs":[],"source":["OKABE_ITO = [\n","    (0,114,178), (230,159,0), (0,158,115), (213,94,0), (86,180,233), (240,228,66), (204,121,167)\n","]\n","\n","def pick_palette(cvd_type):\n","    if cvd_type.lower() in (\"protan\",\"deutan\"):\n","        order = [0,1,6,4,5]\n","    elif cvd_type.lower()==\"tritan\":\n","        order = [3,2,6,1,0]\n","    elif cvd_type.lower()==\"custom\":\n","        order = [1,3,6]  # TODO (Exercise 1): tweak your own set\n","    else:\n","        order = list(range(len(OKABE_ITO)))\n","    return [OKABE_ITO[i] for i in order]\n","\n","def segment_from_boxes(image_rgb, boxes_xyxy):\n","    sam_predictor.set_image(image_rgb)\n","    if len(boxes_xyxy)==0: return []\n","    boxes = np.array(boxes_xyxy)\n","    tb = sam_predictor.transform.apply_boxes_torch(torch.from_numpy(boxes), image_rgb.shape[:2])\n","    with torch.no_grad():\n","        masks, scores, _ = sam_predictor.predict_torch(point_coords=None, point_labels=None, boxes=tb.to(sam_predictor.device), multimask_output=False)\n","    return (masks.squeeze(1).cpu().numpy()>0.5).astype(np.uint8)\n","\n","def overlay_masks(image_bgr, masks, boxes, cvd_type=\"deutan\", alpha=0.45):\n","    out = image_bgr.copy()\n","    palette = pick_palette(cvd_type)\n","    for i, m in enumerate(masks):\n","        color = palette[i % len(palette)]\n","        fill = np.zeros_like(out); fill[m.astype(bool)] = np.array(color, dtype=np.uint8)\n","        out = cv2.addWeighted(out, 1.0, fill, alpha, 0)\n","        edges = cv2.morphologyEx(m, cv2.MORPH_GRADIENT, np.ones((5,5), np.uint8))\n","        out[edges>0] = (255,255,255)\n","    for i, box in enumerate(boxes):\n","        color = palette[i % len(palette)]\n","        cv2.rectangle(out, (int(box[0]),int(box[1])), (int(box[2]),int(box[3])), color, 3)\n","    lab = cv2.cvtColor(out, cv2.COLOR_BGR2LAB)\n","    l,a,b = cv2.split(lab)\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","    l2 = clahe.apply(l)\n","    return cv2.cvtColor(cv2.merge([l2,a,b]), cv2.COLOR_LAB2BGR)\n","\n","def describe_angle(box, W, H):\n","    # TODO (Exercise 2): try zone-based wording\n","    cx = (box[0]+box[2])/2; cy=(box[1]+box[3])/2\n","    dx, dy = cx - W/2, H/2 - cy\n","    ang = (np.degrees(np.arctan2(dy, dx)) + 360) % 360\n","    hours = int(((ang + 15) % 360)//30) or 12\n","    return f\"{hours} o'clock\"\n","\n","from gtts import gTTS\n","import tempfile, json\n","\n","def tts_file(text, speed=1.0, lang=\"en\"):\n","    try:\n","        tts = gTTS(text=text, lang=lang, slow=(speed < 1.0))\n","    except Exception as e:\n","        print(\"TTS failed:\", e)\n","        return None"]},{"cell_type":"markdown","id":"bf99e176","metadata":{"id":"bf99e176"},"source":["## 6. Gradio UI (fast path, OmDet‑Turbo by default)"]},{"cell_type":"code","execution_count":null,"id":"2a6b8f9c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2a6b8f9c","executionInfo":{"status":"ok","timestamp":1761218815447,"user_tz":-120,"elapsed":239637,"user":{"displayName":"Yutong Zhou","userId":"15287789925998213235"}},"outputId":"9a6fbed6-6173-46d2-d45a-cc261c5808f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","* To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7862, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["ERROR:    Exception in ASGI application\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n","    result = await app(  # type: ignore[func-returns-value]\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n","    return await self.app(scope, receive, send)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n","    await super().__call__(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n","    await self.middleware_stack(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n","    raise exc\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n","    await self.app(scope, receive, _send)\n","  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n","    return await self.app(scope, receive, send)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n","    await self.app(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n","    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n","    raise exc\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n","    await app(scope, receive, sender)\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n","    await self.app(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n","    await self.middleware_stack(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n","    await route.handle(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n","    await self.app(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n","    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n","    raise exc\n","  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n","    await app(scope, receive, sender)\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n","    response = await f(request)\n","               ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 389, in app\n","    raw_response = await run_endpoint_function(\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n","    return await dependant.call(**values)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n","    await asyncio.wait_for(\n","  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n","    return await fut\n","           ^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n","    return await self._signals[upload_id].wait()\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n","    fut = self._get_loop().create_future()\n","          ^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n","    raise RuntimeError(f'{self!r} is bound to a different event loop')\n","RuntimeError: <asyncio.locks.Event object at 0x79c2383692e0 [unset]> is bound to a different event loop\n"]},{"output_type":"stream","name":"stdout","text":["Keyboard interruption in main thread... closing server.\n","UI ready. Select OmDet‑Turbo for the fastest path.\n"]}],"source":["%pip -q install nest_asyncio gradio\n","import gradio as gr, time, json\n","\n","def detect_router(image_rgb, prompt_text, backend, box_thr, text_thr):\n","    if backend==\"omdet-turbo\":\n","        return omdet_detect(image_rgb, prompt_text, box_threshold=box_thr)\n","    elif backend==\"florence2\":\n","        return florence2_detect(image_rgb, prompt_text)\n","    else:\n","        bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n","        return gdino_detect(bgr, prompt_text, box_threshold=box_thr, text_threshold=text_thr)\n","\n","def run_pipeline(img, prompt_text, backend, cvd_type, simulate, tts, tts_speed, box_thr, text_thr, low_vision):\n","    if img is None or not prompt_text.strip():\n","        return None, None, \"{}\"\n","    img_rgb = img\n","    if simulate!=\"none\":\n","        img_rgb = simulate_cvd(img_rgb, simulate)\n","    if low_vision:\n","        img_rgb = cv2.GaussianBlur(img_rgb, (21,21), 8)\n","\n","    t0 = time.time()\n","    boxes, scores, labels = detect_router(img_rgb, prompt_text, backend, box_thr, text_thr)\n","    det_ms = (time.time()-t0)*1000\n","\n","    masks = segment_from_boxes(img_rgb, boxes)\n","    out_bgr = overlay_masks(cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR), masks, boxes, cvd_type=cvd_type, alpha=0.45)\n","    out_rgb = cv2.cvtColor(out_bgr, cv2.COLOR_BGR2RGB)\n","\n","    audio_path = None\n","    W,H = img_rgb.shape[1], img_rgb.shape[0]\n","    records = []\n","    for i, box in enumerate(boxes):\n","        label = labels[i % len(labels)] if labels else \"object\"\n","        angle = describe_angle(box, W, H)\n","        score = float(scores[i]) if len(scores)>i else 1.0\n","        records.append({\"label\":label, \"box_xyxy\":[int(v) for v in box], \"angle\":angle, \"score\":score})\n","    if tts and len(records)>0:\n","        phrases = [f\"{r['label']} at {r['angle']}\" for r in records]\n","        audio_path = tts_file(\". \".join(phrases), speed=tts_speed or 1.0)\n","\n","    audio_path = None\n","    if tts and len(records) > 0:\n","        try:\n","            phrases = [f\"{r['label']} at {r['angle']}\" for r in records]\n","            audio_path = tts_file(\". \".join(phrases),\n","                              speed=tts_speed or 1.0)\n","        except Exception as e:\n","            print(\"TTS failed:\", e)\n","            audio_path = None\n","\n","    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n","    img_out = f\"annotated_{ts}.png\"\n","    json_out = f\"detections_{ts}.json\"\n","    aud_out = f\"audio_callouts_{ts}.mp3\"\n","\n","    # Save outputs\n","    # cv2.imwrite(\"annotated.png\", cv2.cvtColor(out_rgb, cv2.COLOR_RGB2BGR))\n","    # with open(\"detections.json\",\"w\") as f:\n","    #     f.write(json.dumps({\"detections\":records, \"meta\":{\"backend\":backend,\"cvd\":cvd_type,\"sim\":simulate,\"latency_ms\":det_ms}}, indent=2))\n","\n","    cv2.imwrite(img_out, cv2.cvtColor(out_rgb, cv2.COLOR_RGB2BGR))\n","    with open(json_out,\"w\") as f:\n","         f.write(json.dumps({\"detections\":records, \"meta\":{\"backend\":backend,\"cvd\":cvd_type,\"sim\":simulate,\"latency_ms\":det_ms}}, indent=2))\n","    audio_path = tts_file(\". \".join(phrases), speed=tts_speed or 1.0, out_path=aud_out)\n","\n","    return out_rgb, (audio_path if audio_path else None), json.dumps({\"detections\":records, \"latency_ms\":det_ms}, indent=2)\n","\n","with gr.Blocks(title=\"Accessible AI: Seeing Through Another’s Eyes\") as demo:\n","    gr.Markdown(\"### Step 1: Feel the limitation → Step 2: Assist with AI → Step 3: Create accessibility\")\n","    with gr.Row():\n","        with gr.Column():\n","            in_img = gr.Image(label=\"Upload or webcam\", sources=[\"upload\",\"webcam\"], type=\"numpy\")\n","            prompt = gr.Textbox(label=\"Text prompts (comma‑separated)\", value=\"sofa, bottle, table, carpet\")\n","            backend = gr.Radio(choices=[\"omdet-turbo\",\"florence2\",\"grounding-dino\"], value=\"omdet-turbo\", label=\"Detector backend\")\n","            cvd = gr.Radio(choices=[\"none\",\"protan (Red)\",\"deutan (Green)\",\"tritan (Blue)\",\"custom\"], value=\"deutan\", label=\"CVD overlay\")\n","            simulate = gr.Radio(choices=[\"none\",\"protan (Red)\",\"deutan (Green)\",\"tritan (Blue)\"], value=\"none\", label=\"Simulate CVD\")\n","            low_vision = gr.Checkbox(False, label=\"Simulate low‑vision blur\")\n","            tts_chk = gr.Checkbox(False, label=\"Enable audio (English)\")\n","            tts_speed = gr.Slider(0.5, 1.0, value=1.0, step=0.25, label=\"TTS speed\")\n","            box_thr = gr.Slider(0.1, 0.6, value=0.25, step=0.05, label=\"Box threshold\")\n","            text_thr = gr.Slider(0.1, 0.6, value=0.25, step=0.05, label=\"Text threshold (DINO only)\")\n","            run_btn = gr.Button(\"Run\")\n","        with gr.Column():\n","            out_img = gr.Image(label=\"AI-assisted view (overlay)\", height=430)\n","            out_audio = gr.Audio(label=\"Audio callouts\", type=\"filepath\")\n","            out_json = gr.JSON(label=\"Detections JSON\")\n","            gr.Markdown(\"**Exports**: `annotated.png`, `detections.json`, `audio_callouts.mp3`\")\n","    run_btn.click(run_pipeline, inputs=[in_img, prompt, backend, cvd, simulate, tts_chk, tts_speed, box_thr, text_thr, low_vision],\n","                  outputs=[out_img, out_audio, out_json])\n","\n","demo.launch(share=False, inbrowser=False, debug=True, show_error=True)\n","print(\"UI ready. Select OmDet‑Turbo for the fastest path.\")"]},{"cell_type":"markdown","id":"862859a8","metadata":{"id":"862859a8"},"source":["## 7. Mini‑Exercises\n"]},{"cell_type":"markdown","id":"3bcc6e3b","metadata":{"id":"3bcc6e3b"},"source":["1. **Make colors more legible** *(edit `pick_palette`)*  \n","   - Change the `custom` palette to maximize separability under your chosen CVD type.\n","\n","2. **Change the narration** *(edit `describe_angle` or phrases in `run_pipeline`)*  \n","   - Replace clock‑face with zones: `\"top-left / center / bottom-right\"`.\n","   - Try more helpful wording: `\"Caution: wheelchair at center-left\"`.\n","\n","3. **Low‑vision** *(try in the UI)*  \n","   - Turn on **Low‑vision blur** and see how overlays restore edges and contrast."]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}